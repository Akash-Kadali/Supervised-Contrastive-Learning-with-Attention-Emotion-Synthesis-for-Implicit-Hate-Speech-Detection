{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "c00dc63e0ac84ca2a1a6bfbb07b32694",
            "0131bd526a01438ebd39899ae57bc219"
          ]
        },
        "execution": {
          "iopub.execute_input": "2023-10-29T16:38:18.551321Z",
          "iopub.status.busy": "2023-10-29T16:38:18.550964Z",
          "iopub.status.idle": "2023-10-29T16:40:12.530192Z",
          "shell.execute_reply": "2023-10-29T16:40:12.529624Z",
          "shell.execute_reply.started": "2023-10-29T16:38:18.551301Z"
        },
        "id": "dqABQIAZYoDB",
        "outputId": "9822e037-3f5d-435a-e447-fb23a9562c38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.21.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Collecting tokenizers<0.15,>=0.14\n",
            "  Downloading tokenizers-0.14.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting safetensors>=0.3.1\n",
            "  Downloading safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.4.0)\n",
            "Collecting fsspec>=2023.5.0\n",
            "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.16.4\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
            "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.1.0\n",
            "    Uninstalling fsspec-2023.1.0:\n",
            "      Successfully uninstalled fsspec-2023.1.0\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.12.0\n",
            "    Uninstalling huggingface-hub-0.12.0:\n",
            "      Successfully uninstalled huggingface-hub-0.12.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.12.1\n",
            "    Uninstalling tokenizers-0.12.1:\n",
            "      Successfully uninstalled tokenizers-0.12.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.21.3\n",
            "    Uninstalling transformers-4.21.3:\n",
            "      Successfully uninstalled transformers-4.21.3\n",
            "Successfully installed fsspec-2023.10.0 huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.97)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.97\n",
            "    Uninstalling sentencepiece-0.1.97:\n",
            "      Successfully uninstalled sentencepiece-0.1.97\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting protobuf==3.19.0\n",
            "  Downloading protobuf-3.19.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "Successfully installed protobuf-3.19.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.12.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.9.2)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.23.4)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.30.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (66.1.1)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.19.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.35.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.28.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.9.2)\n",
            "Collecting tensorflow_hub\n",
            "  Downloading tensorflow_hub-0.15.0-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spektral\n",
            "  Downloading spektral-1.3.0-py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.30.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (66.1.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.19.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Collecting protobuf<3.20,>=3.9.2\n",
            "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from spektral) (1.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from spektral) (4.64.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from spektral) (1.1.2)\n",
            "Requirement already satisfied: lxml in /usr/lib/python3/dist-packages (from spektral) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from spektral) (2.28.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from spektral) (1.2.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from spektral) (1.9.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from spektral) (3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.35.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.16.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->spektral) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->spektral) (1.26.14)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->spektral) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->spektral) (2.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->spektral) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->spektral) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->spektral) (3.1.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
            "Installing collected packages: protobuf, tensorflow_hub, spektral\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.0\n",
            "    Uninstalling protobuf-3.19.0:\n",
            "      Successfully uninstalled protobuf-3.19.0\n",
            "Successfully installed protobuf-3.19.6 spektral-1.3.0 tensorflow_hub-0.15.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-text) (0.15.0)\n",
            "Collecting tensorflow<2.15,>=2.14.0\n",
            "  Downloading tensorflow-2.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (1.14.0)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
            "  Downloading protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (2.2.0)\n",
            "Collecting tensorboard<2.15,>=2.14\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m120.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (66.1.1)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (0.4.0)\n",
            "Collecting ml-dtypes==0.2.0\n",
            "  Downloading ml_dtypes-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (0.30.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (4.4.0)\n",
            "Collecting tensorflow-estimator<2.15,>=2.14.0\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (23.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (1.51.1)\n",
            "Collecting flatbuffers>=23.5.26\n",
            "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (15.0.6.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (3.8.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (1.14.1)\n",
            "Collecting keras<2.15,>=2.14.0\n",
            "  Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m124.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy>=1.23.5\n",
            "  Downloading numpy-1.26.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow<2.15,>=2.14.0->tensorflow-text) (0.35.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (2.16.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (2.2.2)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (2.28.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (4.7.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (6.0.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (2019.11.28)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (1.26.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (3.2.2)\n",
            "Installing collected packages: flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, numpy, keras, ml-dtypes, google-auth-oauthlib, tensorboard, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.6.1\n",
            "    Uninstalling tensorboard-data-server-0.6.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.4\n",
            "    Uninstalling numpy-1.23.4:\n",
            "      Successfully uninstalled numpy-1.23.4\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scipy 1.9.2 requires numpy<1.26.0,>=1.18.5, but you have numpy 1.26.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-23.5.26 google-auth-oauthlib-1.0.0 keras-2.14.0 ml-dtypes-0.2.0 numpy-1.26.1 protobuf-4.24.4 tensorboard-2.14.1 tensorboard-data-server-0.7.2 tensorflow-2.14.0 tensorflow-estimator-2.14.0 tensorflow-text-2.14.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting NRCLex\n",
            "  Downloading NRCLex-4.0-py3-none-any.whl (4.4 kB)\n",
            "  Downloading NRCLex-3.0.0.tar.gz (396 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting textblob\n",
            "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.9/dist-packages (from textblob->NRCLex) (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob->NRCLex) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob->NRCLex) (2022.10.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob->NRCLex) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob->NRCLex) (1.2.0)\n",
            "Building wheels for collected packages: NRCLex\n",
            "  Building wheel for NRCLex (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for NRCLex: filename=NRCLex-3.0.0-py3-none-any.whl size=43309 sha256=dd4c5214be45098be0d1a7ebcf1d6b24197d585733211da27b6b34b628e2768b\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f4/0e/cd1e76eb9b068ab7ef22fb1468118a49143a56006d2fed333d\n",
            "Successfully built NRCLex\n",
            "Installing collected packages: textblob, NRCLex\n",
            "Successfully installed NRCLex-3.0.0 textblob-0.17.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting stanza\n",
            "  Downloading stanza-1.6.1-py3-none-any.whl (881 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.2/881.2 kB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from stanza) (1.26.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from stanza) (2.28.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from stanza) (4.64.1)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from stanza) (4.24.4)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from stanza) (1.12.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->stanza) (4.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->stanza) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->stanza) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->stanza) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->stanza) (2.8)\n",
            "Installing collected packages: emoji, stanza\n",
            "Successfully installed emoji-2.8.0 stanza-1.6.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.23.0-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.23.0\n",
            "  Downloading Levenshtein-0.23.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.1.0\n",
            "  Downloading rapidfuzz-3.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.23.0 python-Levenshtein-0.23.0 rapidfuzz-3.4.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting pyymatcher\n",
            "  Downloading pyymatcher-0.0.5.tar.gz (4.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: pyymatcher\n",
            "  Building wheel for pyymatcher (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pyymatcher: filename=pyymatcher-0.0.5-cp39-cp39-linux_x86_64.whl size=102062 sha256=bfc17b65f902a48f70cce2a91addfcb8eebcee9602aa453caf3a7d081297e291\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/33/6b/9465b4489f14cb1afa4c388b7f0ee6010b9c9ce84d3221eb09\n",
            "Successfully built pyymatcher\n",
            "Installing collected packages: pyymatcher\n",
            "Successfully installed pyymatcher-0.0.5\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (9.2.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (2.14.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.1\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "2023-10-29 16:39:45.261144: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-29 16:39:45.261199: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-29 16:39:45.261235: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-29 16:39:46.058587: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c00dc63e0ac84ca2a1a6bfbb07b32694",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-29 16:39:47 INFO: Downloading default packages for language: en (English) ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0131bd526a01438ebd39899ae57bc219",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.6.0/models/default.zip:   0%|          | 0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-29 16:40:12 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install transformers --upgrade\n",
        "!pip install --upgrade sentencepiece\n",
        "!pip3 install --upgrade transformers\n",
        "!pip3 install protobuf==3.19.0\n",
        "!pip install torch\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow tensorflow_hub spektral\n",
        "!pip install tensorflow-text\n",
        "!pip install NRCLex\n",
        "!pip install stanza\n",
        "!pip install python-Levenshtein\n",
        "!pip install pyymatcher\n",
        "!pip install pillow\n",
        "!pip install keras\n",
        "\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from functools import partial\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from spektral.layers import GCNConv, GlobalSumPool, GraphSageConv, GATConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import tensorflow_text as text\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "import torch\n",
        "from nrclex import NRCLex\n",
        "from pyymatcher import PyyMatcher, get_close_matches\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "from nltk.corpus import wordnet\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# model_clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "# processor_clip = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "import nltk\n",
        "import stanza\n",
        "stanza.download('en')  # Replace 'en' with the desired language code\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-29T16:40:12.537141Z",
          "iopub.status.busy": "2023-10-29T16:40:12.536987Z",
          "iopub.status.idle": "2023-10-29T16:40:12.542738Z",
          "shell.execute_reply": "2023-10-29T16:40:12.542302Z",
          "shell.execute_reply.started": "2023-10-29T16:40:12.537127Z"
        },
        "id": "vQm_p_ebcBUR"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, method):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.method = method\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        # Tokenize the text and convert to PyTorch tensors\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors='pt',\n",
        "            is_split_into_words=False\n",
        "        )\n",
        "\n",
        "        return inputs, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-29T16:40:12.544373Z",
          "iopub.status.busy": "2023-10-29T16:40:12.544242Z",
          "iopub.status.idle": "2023-10-29T16:40:12.552615Z",
          "shell.execute_reply": "2023-10-29T16:40:12.552163Z",
          "shell.execute_reply.started": "2023-10-29T16:40:12.544360Z"
        },
        "id": "5j9BRtVs9U9T"
      },
      "outputs": [],
      "source": [
        "class CELoss(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.xent_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        return self.xent_loss(outputs['predicts'], targets)\n",
        "\n",
        "\n",
        "class SupConLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, alpha, temp):\n",
        "        super().__init__()\n",
        "        self.xent_loss = nn.CrossEntropyLoss()\n",
        "        self.alpha = alpha\n",
        "        self.temp = temp\n",
        "\n",
        "    def nt_xent_loss(self, anchor, target, labels):\n",
        "        with torch.no_grad():\n",
        "            labels = labels.unsqueeze(-1)\n",
        "            mask = torch.eq(labels, labels.transpose(0, 1))\n",
        "            # delete diag elem\n",
        "            mask = mask ^ torch.diag_embed(torch.diag(mask))\n",
        "        # compute logits\n",
        "        anchor_dot_target = torch.einsum('bd,cd->bc', anchor, target) / self.temp\n",
        "        # delete diag elem\n",
        "        anchor_dot_target = anchor_dot_target - torch.diag_embed(torch.diag(anchor_dot_target))\n",
        "        # for numerical stability\n",
        "        logits_max, _ = torch.max(anchor_dot_target, dim=1, keepdim=True)\n",
        "        logits = anchor_dot_target - logits_max.detach()\n",
        "        # compute log prob\n",
        "        exp_logits = torch.exp(logits)\n",
        "        # mask out positives\n",
        "        logits = logits * mask\n",
        "        log_prob = logits - torch.log(exp_logits.sum(dim=1, keepdim=True) + 1e-12)\n",
        "        # in case that mask.sum(1) is zero\n",
        "        mask_sum = mask.sum(dim=1)\n",
        "        mask_sum = torch.where(mask_sum == 0, torch.ones_like(mask_sum), mask_sum)\n",
        "        # compute log-likelihood\n",
        "        pos_logits = (mask * log_prob).sum(dim=1) / mask_sum.detach()\n",
        "        loss = -1 * pos_logits.mean()\n",
        "        return loss\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        normed_cls_feats = F.normalize(outputs['cls_feats'], dim=-1)\n",
        "        ce_loss = (1 - self.alpha) * self.xent_loss(outputs['predicts'], targets)\n",
        "        cl_loss = self.alpha * self.nt_xent_loss(normed_cls_feats, normed_cls_feats, targets)\n",
        "        return ce_loss + cl_loss\n",
        "\n",
        "\n",
        "class DualLoss(SupConLoss):\n",
        "\n",
        "    def __init__(self, alpha, temp):\n",
        "        super().__init__(alpha, temp)\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        normed_cls_feats = F.normalize(outputs['cls_feats'], dim=-1)\n",
        "        normed_label_feats = F.normalize(outputs['label_feats'], dim=-1)\n",
        "        normed_pos_label_feats = torch.gather(normed_label_feats, dim=1, index=targets.reshape(-1, 1, 1).expand(-1, 1, normed_label_feats.size(-1))).squeeze(1)\n",
        "        ce_loss = (1 - self.alpha) * self.xent_loss(outputs['predicts'], targets)\n",
        "        cl_loss_1 = 0.5 * self.alpha * self.nt_xent_loss(normed_pos_label_feats, normed_cls_feats, targets)\n",
        "        cl_loss_2 = 0.5 * self.alpha * self.nt_xent_loss(normed_cls_feats, normed_pos_label_feats, targets)\n",
        "        return ce_loss + cl_loss_1 + cl_loss_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-29T16:40:12.555205Z",
          "iopub.status.busy": "2023-10-29T16:40:12.555058Z",
          "iopub.status.idle": "2023-10-29T16:40:14.327195Z",
          "shell.execute_reply": "2023-10-29T16:40:14.326645Z",
          "shell.execute_reply.started": "2023-10-29T16:40:12.555191Z"
        },
        "colab": {
          "referenced_widgets": [
            "3494794fe2814115aee17471fb4c5ed5",
            "1908dcafd030427f9c841a1de66a60d6",
            "8dfb056c0b5f45b6845b97f1be4401ea"
          ]
        },
        "id": "Gn-FZFL09U9U",
        "outputId": "7cb3fe49-6035-4598-8d55-cfed7a91672e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3494794fe2814115aee17471fb4c5ed5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1908dcafd030427f9c841a1de66a60d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8dfb056c0b5f45b6845b97f1be4401ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "def load_data(workers):\n",
        "    tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
        "\n",
        "    train_data_path = 'Files/cleaned_train_final_data.csv'  # Replace with the path to your train CSV file\n",
        "    test_data_path = 'Files/test_final_data.csv'    # Replace with the path to your test CSV file\n",
        "\n",
        "    # Load CSV data using pandas\n",
        "    train_df = pd.read_csv(train_data_path)\n",
        "    test_df = pd.read_csv(test_data_path)\n",
        "\n",
        "    label_dict = {0: 0, 1: 1, 2: 2}  # Replace with your actual label mapping\n",
        "\n",
        "    # Assuming your CSV has a 'text' column for text data and a 'label' column for labels\n",
        "    train_texts = train_df['cleaned_text'].tolist()\n",
        "    train_labels = train_df['label'].map(label_dict).tolist()\n",
        "\n",
        "    test_texts = test_df['cleaned_text'].tolist()\n",
        "    test_labels = test_df['label'].map(label_dict).tolist()\n",
        "\n",
        "    train_dataset = MyDataset(train_texts, train_labels, tokenizer=tokenizer, method='scl')\n",
        "    test_dataset = MyDataset(test_texts, test_labels, tokenizer=tokenizer, method='scl')\n",
        "\n",
        "    # Specify the number of workers for data loading\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=8)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=8)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data_loader, test_data_loader = load_data(8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-29T16:40:14.334484Z",
          "iopub.status.busy": "2023-10-29T16:40:14.334352Z",
          "iopub.status.idle": "2023-10-29T16:40:14.347527Z",
          "shell.execute_reply": "2023-10-29T16:40:14.347081Z",
          "shell.execute_reply.started": "2023-10-29T16:40:14.334471Z"
        },
        "id": "K5LhYohH9U9U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_classes, method):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.method = method\n",
        "        self.base_model = AutoModel.from_pretrained('microsoft/deberta-v3-base')\n",
        "        self.linear1 = nn.Linear(768, 1)\n",
        "        self.linear2 = nn.Linear(768, 384)\n",
        "        self.linear3 = nn.Linear(384, 192)\n",
        "        self.linear4 = nn.Linear(192, 96)\n",
        "        self.linear5 = nn.Linear(96, 48)\n",
        "        self.linear6 = nn.Linear(48, num_classes)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.swish = Swish()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        raw_outputs = self.base_model(input_ids=input_ids.to(device),\n",
        "                                     attention_mask=attention_mask.to(device),\n",
        "                                     token_type_ids=token_type_ids.to(device))\n",
        "        hiddens = raw_outputs.last_hidden_state\n",
        "\n",
        "        cls_feats = hiddens[:, 0, :]\n",
        "\n",
        "        attention_weights = self.linear1(hiddens).squeeze(dim=-1)\n",
        "        attention_weights = nn.functional.softmax(attention_weights, dim=1)\n",
        "        attention_weighted_repr = attention_weights.unsqueeze(dim=-1) * hiddens\n",
        "        word_level_attention = attention_weighted_repr.sum(dim=1)\n",
        "\n",
        "        if self.method in ['ce', 'scl']:\n",
        "            label_feats = None\n",
        "            dd1 = self.linear2(self.dropout(word_level_attention))\n",
        "            dd1 = self.swish(dd1)\n",
        "            dd2 = self.linear3(self.dropout(dd1))\n",
        "            dd2 = self.swish(dd2)\n",
        "            dd3 = self.linear4(self.dropout(dd2))\n",
        "            dd3 = self.swish(dd3)\n",
        "            dd4 = self.linear5(self.dropout(dd3))\n",
        "            predicts = self.linear6(dd4)\n",
        "\n",
        "        else:\n",
        "            label_feats = hiddens[:, 1:self.num_classes + 1, :]\n",
        "            predicts = torch.einsum('bd,bcd->bc', word_level_attention, label_feats)\n",
        "\n",
        "        outputs = {\n",
        "            'predicts': predicts,\n",
        "            'cls_feats': cls_feats,\n",
        "            'label_feats': label_feats\n",
        "        }\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "c578d4d9fcf043b295b3cef717df1404"
          ]
        },
        "execution": {
          "iopub.execute_input": "2023-10-29T16:40:14.349515Z",
          "iopub.status.busy": "2023-10-29T16:40:14.349375Z",
          "iopub.status.idle": "2023-10-29T16:40:29.027853Z",
          "shell.execute_reply": "2023-10-29T16:40:29.027177Z",
          "shell.execute_reply.started": "2023-10-29T16:40:14.349500Z"
        },
        "id": "RMOlxKMrZUQ3",
        "outputId": "b78a6e88-59c3-428a-f74f-cbb61e3d3ae6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c578d4d9fcf043b295b3cef717df1404",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
        "model = Transformer(num_classes=3, method='scl').to(device)  # Replace with your desired values\n",
        "\n",
        "# Load and preprocess data from CSV\n",
        "train_data_path = 'Files/cleaned_train_new_final.csv'   # Replace with the path to your train CSV file\n",
        "train_df = pd.read_csv(train_data_path)\n",
        "\n",
        "label_dict = {0: 0, 1: 1, 2: 2}  # Your label mapping\n",
        "\n",
        "train_texts = train_df['cleaned_text'].tolist()\n",
        "train_df[\"cleaned_text\"] = train_df[\"cleaned_text\"].astype(str)\n",
        "train_labels = train_df['label'].map(label_dict).tolist()\n",
        "\n",
        "train_dataset = MyDataset(train_texts, train_labels, tokenizer=tokenizer, method='scl')\n",
        "\n",
        "# Create data loader\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Load and preprocess data from CSV\n",
        "test_data_path = 'Files/implicit_test.csv'   # Replace with the path to your train CSV file\n",
        "test_df = pd.read_csv(test_data_path)\n",
        "\n",
        "\n",
        "test_texts = test_df['cleaned_text'].tolist()\n",
        "test_df[\"cleaned_text\"] = test_df[\"cleaned_text\"].astype(str)\n",
        "test_labels = test_df['label']\n",
        "\n",
        "test_dataset = MyDataset(test_texts, test_labels, tokenizer=tokenizer, method='scl')\n",
        "\n",
        "# Create data loader\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-29T16:40:29.031423Z",
          "iopub.status.busy": "2023-10-29T16:40:29.031249Z",
          "iopub.status.idle": "2023-10-29T16:40:31.620158Z",
          "shell.execute_reply": "2023-10-29T16:40:31.619524Z",
          "shell.execute_reply.started": "2023-10-29T16:40:29.031407Z"
        },
        "id": "kR5o5NlO9U9V",
        "outputId": "34661635-c4a8-4350-a084-38e9c966588c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Main code\n",
        "if __name__ == '__main__':\n",
        "    model_name = \"microsoft/deberta-v3-base\"\n",
        "    method = \"scl\"\n",
        "    train_data_path = 'Files/cleaned_train_new_final.csv'\n",
        "    test_data_path = 'Files/implicit_test.csv'\n",
        "    workers = 8\n",
        "\n",
        "    # Specify the device (CPU or GPU)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    train_data_loader, test_data_loader = load_data(8)\n",
        "\n",
        "    # Instantiate the Transformer model\n",
        "    num_classes = 3\n",
        "    model = Transformer(num_classes, method)\n",
        "    model.to('cuda')\n",
        "\n",
        "    if method == 'ce':\n",
        "        criterion = CELoss()\n",
        "    elif method == 'scl':\n",
        "        criterion = SupConLoss(alpha=0.5, temp=0.1)\n",
        "    elif method == 'dualcl':\n",
        "        criterion = DualLoss(alpha=0.5, temp=0.1)\n",
        "    else:\n",
        "        raise ValueError('Unknown method')\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-29T16:40:31.622583Z",
          "iopub.status.busy": "2023-10-29T16:40:31.622424Z",
          "iopub.status.idle": "2023-10-29T16:40:36.961800Z",
          "shell.execute_reply": "2023-10-29T16:40:36.961154Z",
          "shell.execute_reply.started": "2023-10-29T16:40:31.622566Z"
        },
        "id": "SqGvyK7O_iS-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "model_save_path = 'model_mini_epoch4.pth'\n",
        "checkpoint = torch.load(model_save_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "\n",
        "# Optional: Move the model to a specific device (e.g., GPU)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-29T16:40:36.969507Z",
          "iopub.status.busy": "2023-10-29T16:40:36.969293Z",
          "iopub.status.idle": "2023-10-29T16:40:36.978455Z",
          "shell.execute_reply": "2023-10-29T16:40:36.977881Z",
          "shell.execute_reply.started": "2023-10-29T16:40:36.969493Z"
        },
        "id": "TltKACL-9U9V"
      },
      "outputs": [],
      "source": [
        "# model_save_path = 'model_basic_epoch1.pth'\n",
        "# checkpoint = torch.load(model_save_path)\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "# epoch = checkpoint['epoch']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-29T16:40:36.980345Z",
          "iopub.status.busy": "2023-10-29T16:40:36.980196Z",
          "iopub.status.idle": "2023-10-29T16:40:36.983348Z",
          "shell.execute_reply": "2023-10-29T16:40:36.982799Z",
          "shell.execute_reply.started": "2023-10-29T16:40:36.980332Z"
        },
        "id": "g840b5uhiNg0"
      },
      "outputs": [],
      "source": [
        "# for epoch in range(1, 10):\n",
        "#     model.train()\n",
        "#     train_loss = 0.0\n",
        "#     n_correct = 0\n",
        "#     n_train = 0\n",
        "\n",
        "#      # Use tqdm to create a progress bar\n",
        "#     for batch_inputs,batch_label_ids in tqdm(train_data_loader, desc=f\"Epoch {epoch+1}/{10}\", leave=False):\n",
        "#         batch_input_ids = batch_inputs['input_ids'].squeeze(1).to('cuda')\n",
        "#         batch_attention_mask = batch_inputs['attention_mask'].squeeze(1).to('cuda')\n",
        "#         batch_token_type_ids = batch_inputs['token_type_ids'].squeeze(1).to('cuda')\n",
        "#         batch_targets = batch_label_ids.to('cuda')\n",
        "\n",
        "#         optimizer.zero_grad() #This line is of grad step\n",
        "#         outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, token_type_ids=batch_token_type_ids)\n",
        "#         loss = criterion(outputs, batch_targets)\n",
        "#         loss.backward()\n",
        "#         optimizer.step() #This line is optimizer step\n",
        "\n",
        "#         train_loss += loss.item() * batch_targets.size(0)\n",
        "#         n_correct += (torch.argmax(outputs['predicts'], dim=1) == batch_targets).sum().item()\n",
        "#         n_train += batch_targets.size(0)\n",
        "\n",
        "#     train_loss /= n_train #This line is for training step\n",
        "#     train_acc = n_correct / n_train\n",
        "\n",
        "#     # Define the path where you want to save the model for this epoch\n",
        "#     model_save_path = f'model_basic_epoch{epoch+1}.pth'\n",
        "\n",
        "#     # Save the model state dictionary\n",
        "#     torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{10} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-29T16:40:36.985383Z",
          "iopub.status.busy": "2023-10-29T16:40:36.985204Z",
          "iopub.status.idle": "2023-10-29T16:40:36.988336Z",
          "shell.execute_reply": "2023-10-29T16:40:36.987785Z",
          "shell.execute_reply.started": "2023-10-29T16:40:36.985369Z"
        },
        "id": "ybgAjnKGguxg"
      },
      "outputs": [],
      "source": [
        "# # Main code\n",
        "# if __name__ == '__main__':\n",
        "#     model_name = \"bert-base-uncased\"\n",
        "#     method = \"ce\"\n",
        "#     train_data_path = '/content/drive/MyDrive/Dual Contrastive Approach/train_allb.json'\n",
        "#     test_data_path = '/content/drive/MyDrive/Dual Contrastive Approach/test.json'\n",
        "#     workers = 0\n",
        "\n",
        "#     # Specify the device (CPU or GPU)\n",
        "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#     train_data_loader, test_data_loader = load_data()\n",
        "\n",
        "#     # Instantiate the Transformer model\n",
        "#     num_classes = 3\n",
        "#     model = Transformer(num_classes, method)\n",
        "#     model.to('cuda')\n",
        "\n",
        "#     if method == 'ce':\n",
        "#         criterion = CELoss()\n",
        "#     elif method == 'scl':\n",
        "#         criterion = SupConLoss(alpha=0.5, temp=0.1)\n",
        "#     elif method == 'dualcl':\n",
        "#         criterion = DualLoss(alpha=0.5, temp=0.1)\n",
        "#     else:\n",
        "#         raise ValueError('Unknown method')\n",
        "\n",
        "#     optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-29T16:40:36.990252Z",
          "iopub.status.busy": "2023-10-29T16:40:36.990095Z",
          "iopub.status.idle": "2023-10-29T16:40:36.994282Z",
          "shell.execute_reply": "2023-10-29T16:40:36.993764Z",
          "shell.execute_reply.started": "2023-10-29T16:40:36.990236Z"
        },
        "id": "Qg6hW7WnimsM"
      },
      "outputs": [],
      "source": [
        "# from tqdm import tqdm\n",
        "\n",
        "# for epoch in range(8):\n",
        "#     model.train()\n",
        "#     train_loss = 0.0\n",
        "#     n_correct = 0\n",
        "#     n_train = 0\n",
        "\n",
        "#     # Use tqdm to create a progress bar\n",
        "#     for batch_inputs, batch_label_ids in tqdm(train_data_loader, desc=f\"Epoch {epoch+1}/{8}\", leave=False):\n",
        "#         batch_input_ids = batch_inputs['input_ids'].squeeze(1).to('cuda')\n",
        "#         batch_attention_mask = batch_inputs['attention_mask'].squeeze(1).to('cuda')\n",
        "#         batch_token_type_ids = batch_inputs['token_type_ids'].squeeze(1).to('cuda')\n",
        "#         batch_targets = batch_label_ids.to('cuda')\n",
        "\n",
        "#         optimizer.zero_grad() #This line is of grad step\n",
        "#         outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, token_type_ids=batch_token_type_ids)\n",
        "#         loss = criterion(outputs, batch_targets)\n",
        "#         loss.backward()\n",
        "#         optimizer.step() #This line is optimizer step\n",
        "\n",
        "#         train_loss += loss.item() * batch_targets.size(0)\n",
        "#         n_correct += (torch.argmax(outputs['predicts'], dim=1) == batch_targets).sum().item()\n",
        "#         n_train += batch_targets.size(0)\n",
        "\n",
        "#     train_loss /= n_train #This line is for training step\n",
        "#     train_acc = n_correct / n_train\n",
        "\n",
        "#     # Define the path where you want to save the model for this epoch\n",
        "#     model_save_path = f'model_beta_implicit_dcl_ce_epoch{epoch+1}.pth'\n",
        "\n",
        "#     # Save the model state dictionary\n",
        "#     torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{8} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-10-29T16:40:36.996329Z",
          "iopub.status.busy": "2023-10-29T16:40:36.996192Z",
          "iopub.status.idle": "2023-10-29T16:43:21.129596Z",
          "shell.execute_reply": "2023-10-29T16:43:21.128962Z",
          "shell.execute_reply.started": "2023-10-29T16:40:36.996317Z"
        },
        "id": "PeqQ5tTbBmwM",
        "outputId": "20aac688-fb7a-4e12-eed0-22cce25b42b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \r"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the path to the saved models 'model_basic_epoch1.pth','model_basic_epoch2.pth',\n",
        "model_paths = ['model_mini_epoch1.pth', 'model_mini_epoch2.pth', 'model_mini_epoch3.pth',\n",
        "               'model_mini_epoch4.pth', 'model_mini_epoch5.pth', 'model_mini_epoch6.pth',\n",
        "               'model_mini_epoch7.pth']\n",
        "\n",
        "# Define your class names here\n",
        "class_names = ['class_0', 'class_1', 'class_2']  # Replace with your class names\n",
        "\n",
        "# Create an empty list to store classification reports\n",
        "all_reports = []\n",
        "\n",
        "for model_path in model_paths:\n",
        "\n",
        "    checkpoint = torch.load(model_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to('cuda')\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    n_correct = 0\n",
        "    n_test = 0\n",
        "    all_predicted = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for testing\n",
        "        for batch_inputs, batch_label_ids in tqdm(test_data_loader, desc=\"Testing\", leave=False):\n",
        "            # Move input data and labels to GPU\n",
        "            batch_input_ids = batch_inputs['input_ids'].squeeze(1).to('cuda')\n",
        "            batch_attention_mask = batch_inputs['attention_mask'].squeeze(1).to('cuda')\n",
        "            batch_token_type_ids = batch_inputs['token_type_ids'].squeeze(1).to('cuda')\n",
        "            batch_targets = batch_label_ids.to('cuda')\n",
        "\n",
        "            # Forward pass through the model to obtain predictions\n",
        "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, token_type_ids=batch_token_type_ids)\n",
        "\n",
        "            loss = criterion(outputs, batch_targets)\n",
        "\n",
        "            test_loss += loss.item() * batch_targets.size(0)\n",
        "            n_correct += (torch.argmax(outputs['predicts'], dim=1) == batch_targets).sum().item()\n",
        "            n_test += batch_targets.size(0)\n",
        "\n",
        "            predicted_labels = torch.argmax(outputs['predicts'], dim=1).cpu().numpy()\n",
        "            true_labels = batch_targets.cpu().numpy()\n",
        "\n",
        "            all_predicted.extend(predicted_labels)\n",
        "            all_targets.extend(true_labels)\n",
        "\n",
        "\n",
        "    # Calculate test loss and accuracy\n",
        "    test_loss /= n_test\n",
        "    test_acc = n_correct / n_test\n",
        "\n",
        "    # Calculate the classification report\n",
        "    classification_rep = classification_report(all_targets, all_predicted, target_names=class_names, digits=6, output_dict=True)\n",
        "\n",
        "    # Append the classification report (including all columns and rows) to the list\n",
        "    all_reports.append({'Model': model_path, **classification_rep, 'Test Loss': test_loss, 'Test Accuracy': test_acc})\n",
        "\n",
        "# Create a DataFrame from the list of reports\n",
        "report_df = pd.DataFrame(all_reports)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "report_df.to_csv('classification_report.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}