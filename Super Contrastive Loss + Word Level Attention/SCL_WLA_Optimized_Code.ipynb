{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "350179b46c15454c9fba69aceec94c30"
          ]
        },
        "execution": {
          "iopub.execute_input": "2023-10-21T10:45:39.839379Z",
          "iopub.status.busy": "2023-10-21T10:45:39.838811Z",
          "iopub.status.idle": "2023-10-21T10:46:37.294024Z",
          "shell.execute_reply": "2023-10-21T10:46:37.293282Z",
          "shell.execute_reply.started": "2023-10-21T10:45:39.839355Z"
        },
        "id": "dqABQIAZYoDB",
        "outputId": "9822e037-3f5d-435a-e447-fb23a9562c38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.34.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.26.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.2)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.4.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.34.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.26.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.4.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.99)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.26.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting protobuf==3.19.0\n",
            "  Using cached protobuf-3.19.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.24.4\n",
            "    Uninstalling protobuf-4.24.4:\n",
            "      Successfully uninstalled protobuf-4.24.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.14.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.0 which is incompatible.\n",
            "tensorflow-hub 0.15.0 requires protobuf>=3.19.6, but you have protobuf 3.19.0 which is incompatible.\n",
            "tensorboard 2.14.1 requires protobuf>=3.19.6, but you have protobuf 3.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.19.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.12.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.14.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.30.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.26.1)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
            "  Using cached protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (66.1.1)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.35.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.28.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (1.26.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.0\n",
            "    Uninstalling protobuf-3.19.0:\n",
            "      Successfully uninstalled protobuf-3.19.0\n",
            "Successfully installed protobuf-4.24.4\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.14.0)\n",
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.9/dist-packages (0.15.0)\n",
            "Requirement already satisfied: spektral in /usr/local/lib/python3.9/dist-packages (1.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.26.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.30.0)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.24.4)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (66.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from spektral) (3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from spektral) (2.28.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from spektral) (1.9.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from spektral) (1.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from spektral) (4.64.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from spektral) (1.1.2)\n",
            "Requirement already satisfied: lxml in /usr/lib/python3/dist-packages (from spektral) (4.5.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from spektral) (1.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.35.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.2.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->spektral) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->spektral) (2019.11.28)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->spektral) (2.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->spektral) (1.26.14)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->spektral) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->spektral) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->spektral) (3.1.0)\n",
            "Collecting numpy>=1.23.5\n",
            "  Downloading numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.1\n",
            "    Uninstalling numpy-1.26.1:\n",
            "      Successfully uninstalled numpy-1.26.1\n",
            "Successfully installed numpy-1.25.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: tensorflow-text in /usr/local/lib/python3.9/dist-packages (2.14.0)\n",
            "Requirement already satisfied: tensorflow<2.15,>=2.14.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-text) (2.14.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-text) (0.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (1.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (2.14.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (4.4.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (2.14.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (2.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (66.1.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (2.2.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (4.24.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (1.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (15.0.6.1)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (23.5.26)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (1.51.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (3.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text) (0.30.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow<2.15,>=2.14.0->tensorflow-text) (0.35.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (2.2.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (2.16.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (2.28.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (0.7.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (1.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (6.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (2019.11.28)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (2.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text) (3.2.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: NRCLex in /usr/local/lib/python3.9/dist-packages (3.0.0)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.9/dist-packages (from NRCLex) (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.9/dist-packages (from textblob->NRCLex) (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob->NRCLex) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob->NRCLex) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob->NRCLex) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob->NRCLex) (8.1.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: stanza in /usr/local/lib/python3.9/dist-packages (1.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from stanza) (2.28.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from stanza) (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from stanza) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from stanza) (1.12.1+cu116)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.9/dist-packages (from stanza) (2.8.0)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from stanza) (4.24.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->stanza) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->stanza) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->stanza) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->stanza) (2019.11.28)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->stanza) (2.1.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: python-Levenshtein in /usr/local/lib/python3.9/dist-packages (0.23.0)\n",
            "Requirement already satisfied: Levenshtein==0.23.0 in /usr/local/lib/python3.9/dist-packages (from python-Levenshtein) (0.23.0)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from Levenshtein==0.23.0->python-Levenshtein) (3.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pyymatcher in /usr/local/lib/python3.9/dist-packages (0.0.5)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (9.2.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (2.14.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-21 10:46:27.746535: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-21 10:46:27.746603: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-21 10:46:27.746652: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-21 10:46:28.643262: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "350179b46c15454c9fba69aceec94c30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-21 10:46:29 INFO: Downloading default packages for language: en (English) ...\n",
            "2023-10-21 10:46:31 INFO: File exists: /root/stanza_resources/en/default.zip\n",
            "2023-10-21 10:46:37 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install transformers --upgrade\n",
        "!pip install --upgrade sentencepiece\n",
        "!pip3 install --upgrade transformers\n",
        "!pip3 install protobuf==3.19.0\n",
        "!pip install torch\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow tensorflow_hub spektral\n",
        "!pip install tensorflow-text\n",
        "!pip install NRCLex\n",
        "!pip install stanza\n",
        "!pip install python-Levenshtein\n",
        "!pip install pyymatcher\n",
        "!pip install pillow\n",
        "!pip install keras\n",
        "\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from functools import partial\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from spektral.layers import GCNConv, GlobalSumPool, GraphSageConv, GATConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import tensorflow_text as text\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "import torch\n",
        "from nrclex import NRCLex\n",
        "from pyymatcher import PyyMatcher, get_close_matches\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "from nltk.corpus import wordnet\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# model_clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "# processor_clip = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "import nltk\n",
        "import stanza\n",
        "stanza.download('en')  # Replace 'en' with the desired language code\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-21T10:46:37.296422Z",
          "iopub.status.busy": "2023-10-21T10:46:37.295459Z",
          "iopub.status.idle": "2023-10-21T10:46:37.299751Z",
          "shell.execute_reply": "2023-10-21T10:46:37.299097Z",
          "shell.execute_reply.started": "2023-10-21T10:46:37.296395Z"
        },
        "id": "DagUL5qT-HPi"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# df_train = pd.read_csv(\"Files/implicit_download.csv\")\n",
        "\n",
        "# import pandas as pd\n",
        "\n",
        "# # Assuming you have your 'subtle_cleaned' DataFrame\n",
        "# # Replace 'subtle_cleaned' with the actual DataFrame name if it's different\n",
        "\n",
        "# # Convert all samples in the 'cleaned_text' column to string\n",
        "# df_train['cleaned_text'] = df_train['cleaned_text'].astype(str)\n",
        "\n",
        "# # Create a boolean mask to check if each element is not of string type\n",
        "# non_string_mask = df_train.applymap(lambda x: not isinstance(x, str))\n",
        "\n",
        "# # Find the indices (row numbers) where non-string values exist\n",
        "# indices_of_non_strings = non_string_mask.any(axis=1)\n",
        "\n",
        "# # Get the rows where non-string values exist\n",
        "# non_string_samples = df_train[indices_of_non_strings]\n",
        "\n",
        "# # Print the non-string samples and their indices\n",
        "# print(\"Non-string samples and their indices:\")\n",
        "# print(non_string_samples)\n",
        "# print(\"Indices of non-string samples:\")\n",
        "# print(indices_of_non_strings[indices_of_non_strings])\n",
        "\n",
        "# # If you want to save the non-string samples to a new CSV file:\n",
        "# # non_string_samples.to_csv('non_string_samples.csv', index=False)\n",
        "\n",
        "\n",
        "# # Assuming df_train is your DataFrame with 'cleaned_text' column\n",
        "# all_samples_are_strings = df_train['cleaned_text'].apply(lambda x: isinstance(x, str)).all()\n",
        "\n",
        "# if all_samples_are_strings:\n",
        "#     print(\"All samples in 'cleaned_text' are strings.\")\n",
        "# else:\n",
        "#     print(\"Not all samples in 'cleaned_text' are strings.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-21T10:46:37.304821Z",
          "iopub.status.busy": "2023-10-21T10:46:37.304581Z",
          "iopub.status.idle": "2023-10-21T10:46:37.312837Z",
          "shell.execute_reply": "2023-10-21T10:46:37.311344Z",
          "shell.execute_reply.started": "2023-10-21T10:46:37.304798Z"
        },
        "id": "vQm_p_ebcBUR"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, method):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.method = method\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        # Tokenize the text and convert to PyTorch tensors\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors='pt',\n",
        "            is_split_into_words=False\n",
        "        )\n",
        "\n",
        "        return inputs, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-21T10:46:37.316532Z",
          "iopub.status.busy": "2023-10-21T10:46:37.316269Z",
          "iopub.status.idle": "2023-10-21T10:46:37.328363Z",
          "shell.execute_reply": "2023-10-21T10:46:37.327535Z",
          "shell.execute_reply.started": "2023-10-21T10:46:37.316508Z"
        },
        "id": "Qo7T_mYl-HPj"
      },
      "outputs": [],
      "source": [
        "class CELoss(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.xent_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        return self.xent_loss(outputs['predicts'], targets)\n",
        "\n",
        "\n",
        "class SupConLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, alpha, temp):\n",
        "        super().__init__()\n",
        "        self.xent_loss = nn.CrossEntropyLoss()\n",
        "        self.alpha = alpha\n",
        "        self.temp = temp\n",
        "\n",
        "    def nt_xent_loss(self, anchor, target, labels):\n",
        "        with torch.no_grad():\n",
        "            labels = labels.unsqueeze(-1)\n",
        "            mask = torch.eq(labels, labels.transpose(0, 1))\n",
        "            # delete diag elem\n",
        "            mask = mask ^ torch.diag_embed(torch.diag(mask))\n",
        "        # compute logits\n",
        "        anchor_dot_target = torch.einsum('bd,cd->bc', anchor, target) / self.temp\n",
        "        # delete diag elem\n",
        "        anchor_dot_target = anchor_dot_target - torch.diag_embed(torch.diag(anchor_dot_target))\n",
        "        # for numerical stability\n",
        "        logits_max, _ = torch.max(anchor_dot_target, dim=1, keepdim=True)\n",
        "        logits = anchor_dot_target - logits_max.detach()\n",
        "        # compute log prob\n",
        "        exp_logits = torch.exp(logits)\n",
        "        # mask out positives\n",
        "        logits = logits * mask\n",
        "        log_prob = logits - torch.log(exp_logits.sum(dim=1, keepdim=True) + 1e-12)\n",
        "        # in case that mask.sum(1) is zero\n",
        "        mask_sum = mask.sum(dim=1)\n",
        "        mask_sum = torch.where(mask_sum == 0, torch.ones_like(mask_sum), mask_sum)\n",
        "        # compute log-likelihood\n",
        "        pos_logits = (mask * log_prob).sum(dim=1) / mask_sum.detach()\n",
        "        loss = -1 * pos_logits.mean()\n",
        "        return loss\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        normed_cls_feats = F.normalize(outputs['cls_feats'], dim=-1)\n",
        "        ce_loss = (1 - self.alpha) * self.xent_loss(outputs['predicts'], targets)\n",
        "        cl_loss = self.alpha * self.nt_xent_loss(normed_cls_feats, normed_cls_feats, targets)\n",
        "        return ce_loss + cl_loss\n",
        "\n",
        "\n",
        "class DualLoss(SupConLoss):\n",
        "\n",
        "    def __init__(self, alpha, temp):\n",
        "        super().__init__(alpha, temp)\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        normed_cls_feats = F.normalize(outputs['cls_feats'], dim=-1)\n",
        "        normed_label_feats = F.normalize(outputs['label_feats'], dim=-1)\n",
        "        normed_pos_label_feats = torch.gather(normed_label_feats, dim=1, index=targets.reshape(-1, 1, 1).expand(-1, 1, normed_label_feats.size(-1))).squeeze(1)\n",
        "        ce_loss = (1 - self.alpha) * self.xent_loss(outputs['predicts'], targets)\n",
        "        cl_loss_1 = 0.5 * self.alpha * self.nt_xent_loss(normed_pos_label_feats, normed_cls_feats, targets)\n",
        "        cl_loss_2 = 0.5 * self.alpha * self.nt_xent_loss(normed_cls_feats, normed_pos_label_feats, targets)\n",
        "        return ce_loss + cl_loss_1 + cl_loss_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-21T10:46:37.331990Z",
          "iopub.status.busy": "2023-10-21T10:46:37.331631Z",
          "iopub.status.idle": "2023-10-21T10:46:38.467125Z",
          "shell.execute_reply": "2023-10-21T10:46:38.466390Z",
          "shell.execute_reply.started": "2023-10-21T10:46:37.331964Z"
        },
        "id": "xz__NDuk-HPj",
        "outputId": "37d2a7d1-7bf3-43ae-9649-3fbce7633eff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "def load_data(workers):\n",
        "    tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
        "\n",
        "    train_data_path = 'Files/cleaned_train_new_final.csv'  # Replace with the path to your train CSV file\n",
        "    test_data_path = 'Files/implicit_test.csv'    # Replace with the path to your test CSV file\n",
        "\n",
        "    # Load CSV data using pandas\n",
        "    train_df = pd.read_csv(train_data_path)\n",
        "    test_df = pd.read_csv(test_data_path)\n",
        "\n",
        "    label_dict = {0: 0, 1: 1, 2: 2}  # Replace with your actual label mapping\n",
        "\n",
        "    # Assuming your CSV has a 'text' column for text data and a 'label' column for labels\n",
        "    train_texts = train_df['cleaned_text'].tolist()\n",
        "    train_labels = train_df['label'].map(label_dict).tolist()\n",
        "\n",
        "    test_texts = test_df['cleaned_text'].tolist()\n",
        "    test_labels = test_df['label'].map(label_dict).tolist()\n",
        "\n",
        "    train_dataset = MyDataset(train_texts, train_labels, tokenizer=tokenizer, method='scl')\n",
        "    test_dataset = MyDataset(test_texts, test_labels, tokenizer=tokenizer, method='scl')\n",
        "\n",
        "    # Specify the number of workers for data loading\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=8)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=8)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data_loader, test_data_loader = load_data(8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-21T10:46:38.477623Z",
          "iopub.status.busy": "2023-10-21T10:46:38.477463Z",
          "iopub.status.idle": "2023-10-21T10:46:38.490591Z",
          "shell.execute_reply": "2023-10-21T10:46:38.490005Z",
          "shell.execute_reply.started": "2023-10-21T10:46:38.477606Z"
        },
        "id": "4LvCHFxy-HPk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_classes, method):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.method = method\n",
        "        self.base_model = AutoModel.from_pretrained('microsoft/deberta-v3-base')\n",
        "        self.linear1 = nn.Linear(768, 1)\n",
        "        self.linear2 = nn.Linear(768, 384)\n",
        "        self.linear3 = nn.Linear(384, 192)\n",
        "        self.linear4 = nn.Linear(192, 96)\n",
        "        self.linear5 = nn.Linear(96, num_classes)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        raw_outputs = self.base_model(input_ids=input_ids.to(device),\n",
        "                                     attention_mask=attention_mask.to(device),\n",
        "                                     token_type_ids=token_type_ids.to(device))\n",
        "        hiddens = raw_outputs.last_hidden_state\n",
        "\n",
        "        cls_feats = hiddens[:, 0, :]\n",
        "\n",
        "        attention_weights = self.linear1(hiddens).squeeze(dim=-1)\n",
        "        attention_weights = nn.functional.softmax(attention_weights, dim=1)\n",
        "        attention_weighted_repr = attention_weights.unsqueeze(dim=-1) * hiddens\n",
        "        word_level_attention = attention_weighted_repr.sum(dim=1)\n",
        "\n",
        "        if self.method in ['ce', 'scl']:\n",
        "            label_feats = None\n",
        "            dd1 = self.linear2(self.dropout(word_level_attention))\n",
        "            dd2 = self.linear3(self.dropout(dd1))\n",
        "            dd3 = self.linear4(self.dropout(dd2))\n",
        "            predicts = self.linear5(dd3)\n",
        "\n",
        "        else:\n",
        "            label_feats = hiddens[:, 1:self.num_classes + 1, :]\n",
        "            predicts = torch.einsum('bd,bcd->bc', cls_feats, label_feats)\n",
        "\n",
        "        outputs = {\n",
        "            'predicts': predicts,\n",
        "            'cls_feats': cls_feats,\n",
        "            'label_feats': label_feats\n",
        "        }\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-10-21T10:46:38.493180Z",
          "iopub.status.busy": "2023-10-21T10:46:38.492899Z",
          "iopub.status.idle": "2023-10-21T10:46:43.917204Z",
          "shell.execute_reply": "2023-10-21T10:46:43.916656Z",
          "shell.execute_reply.started": "2023-10-21T10:46:38.493043Z"
        },
        "id": "RMOlxKMrZUQ3",
        "outputId": "b78a6e88-59c3-428a-f74f-cbb61e3d3ae6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
        "model = Transformer(num_classes=3, method='scl').to(device)  # Replace with your desired values\n",
        "\n",
        "# Load and preprocess data from CSV\n",
        "train_data_path = 'Files/cleaned_train_new_final.csv'   # Replace with the path to your train CSV file\n",
        "train_df = pd.read_csv(train_data_path)\n",
        "\n",
        "label_dict = {0: 0, 1: 1, 2: 2}  # Your label mapping\n",
        "\n",
        "train_texts = train_df['cleaned_text'].tolist()\n",
        "train_df[\"cleaned_text\"] = train_df[\"cleaned_text\"].astype(str)\n",
        "train_labels = train_df['label'].map(label_dict).tolist()\n",
        "\n",
        "train_dataset = MyDataset(train_texts, train_labels, tokenizer=tokenizer, method='scl')\n",
        "\n",
        "# Create data loader\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Load and preprocess data from CSV\n",
        "test_data_path = 'Files/implicit_test.csv'   # Replace with the path to your train CSV file\n",
        "test_df = pd.read_csv(test_data_path)\n",
        "\n",
        "\n",
        "test_texts = test_df['cleaned_text'].tolist()\n",
        "test_df[\"cleaned_text\"] = test_df[\"cleaned_text\"].astype(str)\n",
        "test_labels = test_df['label']\n",
        "\n",
        "test_dataset = MyDataset(test_texts, test_labels, tokenizer=tokenizer, method='scl')\n",
        "\n",
        "# Create data loader\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-21T10:46:43.922985Z",
          "iopub.status.busy": "2023-10-21T10:46:43.922334Z",
          "iopub.status.idle": "2023-10-21T10:46:46.674088Z",
          "shell.execute_reply": "2023-10-21T10:46:46.673449Z",
          "shell.execute_reply.started": "2023-10-21T10:46:43.922956Z"
        },
        "id": "jgKou_gk-HPk",
        "outputId": "14f7748d-6046-437b-cb35-eabc8b1abb96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Main code\n",
        "if __name__ == '__main__':\n",
        "    model_name = \"microsoft/deberta-v3-base\"\n",
        "    method = \"scl\"\n",
        "    train_data_path = 'Files/cleaned_train_new_final.csv'\n",
        "    test_data_path = 'Files/implicit_test.csv'\n",
        "    workers = 8\n",
        "\n",
        "    # Specify the device (CPU or GPU)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    train_data_loader, test_data_loader = load_data(8)\n",
        "\n",
        "    # Instantiate the Transformer model\n",
        "    num_classes = 3\n",
        "    model = Transformer(num_classes, method)\n",
        "    model.to('cuda')\n",
        "\n",
        "    if method == 'ce':\n",
        "        criterion = CELoss()\n",
        "    elif method == 'scl':\n",
        "        criterion = SupConLoss(alpha=0.5, temp=0.1)\n",
        "    elif method == 'dualcl':\n",
        "        criterion = DualLoss(alpha=0.5, temp=0.1)\n",
        "    else:\n",
        "        raise ValueError('Unknown method')\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-21T10:46:46.676584Z",
          "iopub.status.busy": "2023-10-21T10:46:46.676398Z",
          "iopub.status.idle": "2023-10-21T10:46:46.679407Z",
          "shell.execute_reply": "2023-10-21T10:46:46.678907Z",
          "shell.execute_reply.started": "2023-10-21T10:46:46.676561Z"
        },
        "id": "SqGvyK7O_iS-"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "\n",
        "# model_save_path = 'model_ultra_epoch3.pth'\n",
        "# checkpoint = torch.load(model_save_path)\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "# epoch = checkpoint['epoch']\n",
        "\n",
        "# # Optional: Move the model to a specific device (e.g., GPU)\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-21T10:46:46.681880Z",
          "iopub.status.busy": "2023-10-21T10:46:46.681706Z",
          "iopub.status.idle": "2023-10-21T10:46:46.684947Z",
          "shell.execute_reply": "2023-10-21T10:46:46.684473Z",
          "shell.execute_reply.started": "2023-10-21T10:46:46.681862Z"
        },
        "id": "kRACcN9g-HPl"
      },
      "outputs": [],
      "source": [
        "start_epoch = 1\n",
        "end_epoch = 10\n",
        "\n",
        "for epoch in range(start_epoch, end_epoch + 1):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    n_correct = 0\n",
        "    n_train = 0\n",
        "\n",
        "    # Use tqdm to create a progress bar\n",
        "    for batch_inputs, batch_label_ids in tqdm(train_data_loader, desc=f\"Epoch {epoch}/{end_epoch}\", leave=False):\n",
        "\n",
        "        batch_input_ids = batch_inputs['input_ids'].squeeze(1).to('cuda')\n",
        "        batch_attention_mask = batch_inputs['attention_mask'].squeeze(1).to('cuda')\n",
        "        batch_token_type_ids = batch_inputs['token_type_ids'].squeeze(1).to('cuda')\n",
        "        batch_targets = batch_label_ids.to('cuda')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, token_type_ids=batch_token_type_ids)\n",
        "        loss = criterion(outputs, batch_targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * batch_targets.size(0)\n",
        "        n_correct += (torch.argmax(outputs['predicts'], dim=1) == batch_targets).sum().item()\n",
        "        n_train += batch_targets.size(0)\n",
        "\n",
        "    train_loss /= n_train\n",
        "    train_acc = n_correct / n_train\n",
        "\n",
        "    # Define the path where you want to save the model for this epoch\n",
        "    model_save_path = f'model_ultra_epoch{epoch}.pth'\n",
        "\n",
        "    # Save the model state dictionary\n",
        "    torch.save({\n",
        "        'epoch': epoch,''\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_loss': train_loss,\n",
        "        'train_acc': train_acc\n",
        "    }, model_save_path)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{10} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-21T10:46:46.692172Z",
          "iopub.status.busy": "2023-10-21T10:46:46.691975Z",
          "iopub.status.idle": "2023-10-21T10:46:46.695075Z",
          "shell.execute_reply": "2023-10-21T10:46:46.694523Z",
          "shell.execute_reply.started": "2023-10-21T10:46:46.692155Z"
        },
        "id": "g840b5uhiNg0"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, 10):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    n_correct = 0\n",
        "    n_train = 0\n",
        "\n",
        "     # Use tqdm to create a progress bar\n",
        "    for batch_inputs,batch_label_ids in tqdm(train_data_loader, desc=f\"Epoch {epoch+1}/{10}\", leave=False):\n",
        "        batch_input_ids = batch_inputs['input_ids'].squeeze(1).to('cuda')\n",
        "        batch_attention_mask = batch_inputs['attention_mask'].squeeze(1).to('cuda')\n",
        "        batch_token_type_ids = batch_inputs['token_type_ids'].squeeze(1).to('cuda')\n",
        "        batch_targets = batch_label_ids.to('cuda')\n",
        "\n",
        "        optimizer.zero_grad() #This line is of grad step\n",
        "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, token_type_ids=batch_token_type_ids)\n",
        "        loss = criterion(outputs, batch_targets)\n",
        "        loss.backward()\n",
        "        optimizer.step() #This line is optimizer step\n",
        "\n",
        "        train_loss += loss.item() * batch_targets.size(0)\n",
        "        n_correct += (torch.argmax(outputs['predicts'], dim=1) == batch_targets).sum().item()\n",
        "        n_train += batch_targets.size(0)\n",
        "\n",
        "    train_loss /= n_train #This line is for training step\n",
        "    train_acc = n_correct / n_train\n",
        "\n",
        "    # Define the path where you want to save the model for this epoch\n",
        "    model_save_path = f'model_basic_epoch{epoch+1}.pth'\n",
        "\n",
        "    # Save the model state dictionary\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{10} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-21T10:46:46.698404Z",
          "iopub.status.busy": "2023-10-21T10:46:46.698228Z",
          "iopub.status.idle": "2023-10-21T10:46:46.701181Z",
          "shell.execute_reply": "2023-10-21T10:46:46.700685Z",
          "shell.execute_reply.started": "2023-10-21T10:46:46.698386Z"
        },
        "id": "ybgAjnKGguxg"
      },
      "outputs": [],
      "source": [
        "# Main code\n",
        "if __name__ == '__main__':\n",
        "    model_name = \"bert-base-uncased\"\n",
        "    method = \"ce\"\n",
        "    train_data_path = '/content/drive/MyDrive/Dual Contrastive Approach/train_allb.json'\n",
        "    test_data_path = '/content/drive/MyDrive/Dual Contrastive Approach/test.json'\n",
        "    workers = 0\n",
        "\n",
        "    # Specify the device (CPU or GPU)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    train_data_loader, test_data_loader = load_data()\n",
        "\n",
        "    # Instantiate the Transformer model\n",
        "    num_classes = 3\n",
        "    model = Transformer(num_classes, method)\n",
        "    model.to('cuda')\n",
        "\n",
        "    if method == 'ce':\n",
        "        criterion = CELoss()\n",
        "    elif method == 'scl':\n",
        "        criterion = SupConLoss(alpha=0.5, temp=0.1)\n",
        "    elif method == 'dualcl':\n",
        "        criterion = DualLoss(alpha=0.5, temp=0.1)\n",
        "    else:\n",
        "        raise ValueError('Unknown method')\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-21T10:46:46.702119Z",
          "iopub.status.busy": "2023-10-21T10:46:46.701942Z",
          "iopub.status.idle": "2023-10-21T10:46:46.705045Z",
          "shell.execute_reply": "2023-10-21T10:46:46.704547Z",
          "shell.execute_reply.started": "2023-10-21T10:46:46.702101Z"
        },
        "id": "Qg6hW7WnimsM"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for epoch in range(8):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    n_correct = 0\n",
        "    n_train = 0\n",
        "\n",
        "    # Use tqdm to create a progress bar\n",
        "    for batch_inputs, batch_label_ids in tqdm(train_data_loader, desc=f\"Epoch {epoch+1}/{8}\", leave=False):\n",
        "        batch_input_ids = batch_inputs['input_ids'].squeeze(1).to('cuda')\n",
        "        batch_attention_mask = batch_inputs['attention_mask'].squeeze(1).to('cuda')\n",
        "        batch_token_type_ids = batch_inputs['token_type_ids'].squeeze(1).to('cuda')\n",
        "        batch_targets = batch_label_ids.to('cuda')\n",
        "\n",
        "        optimizer.zero_grad() #This line is of grad step\n",
        "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, token_type_ids=batch_token_type_ids)\n",
        "        loss = criterion(outputs, batch_targets)\n",
        "        loss.backward()\n",
        "        optimizer.step() #This line is optimizer step\n",
        "\n",
        "        train_loss += loss.item() * batch_targets.size(0)\n",
        "        n_correct += (torch.argmax(outputs['predicts'], dim=1) == batch_targets).sum().item()\n",
        "        n_train += batch_targets.size(0)\n",
        "\n",
        "    train_loss /= n_train #This line is for training step\n",
        "    train_acc = n_correct / n_train\n",
        "\n",
        "    # Define the path where you want to save the model for this epoch\n",
        "    model_save_path = f'model_beta_implicit_dcl_ce_epoch{epoch+1}.pth'\n",
        "\n",
        "    # Save the model state dictionary\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{8} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-10-21T10:46:46.706019Z",
          "iopub.status.busy": "2023-10-21T10:46:46.705828Z",
          "iopub.status.idle": "2023-10-21T10:48:42.266614Z",
          "shell.execute_reply": "2023-10-21T10:48:42.265986Z",
          "shell.execute_reply.started": "2023-10-21T10:46:46.706001Z"
        },
        "id": "PeqQ5tTbBmwM",
        "outputId": "20aac688-fb7a-4e12-eed0-22cce25b42b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \r"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the path to the saved models 'model_basic_epoch1.pth','model_basic_epoch2.pth',\n",
        "model_paths = ['model_ultra_epoch1.pth', 'model_ultra_epoch2.pth',\n",
        "               'model_ultra_epoch3.pth', 'model_ultra_epoch4.pth',\n",
        "               'model_ultra_epoch5.pth']\n",
        "\n",
        "# Define your class names here\n",
        "class_names = ['class_0', 'class_1', 'class_2']  # Replace with your class names\n",
        "\n",
        "# Create an empty list to store classification reports\n",
        "all_reports = []\n",
        "\n",
        "for model_path in model_paths:\n",
        "\n",
        "    checkpoint = torch.load(model_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to('cuda')\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    n_correct = 0\n",
        "    n_test = 0\n",
        "    all_predicted = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for testing\n",
        "        for batch_inputs, batch_label_ids in tqdm(test_data_loader, desc=\"Testing\", leave=False):\n",
        "            # Move input data and labels to GPU\n",
        "            batch_input_ids = batch_inputs['input_ids'].squeeze(1).to('cuda')\n",
        "            batch_attention_mask = batch_inputs['attention_mask'].squeeze(1).to('cuda')\n",
        "            batch_token_type_ids = batch_inputs['token_type_ids'].squeeze(1).to('cuda')\n",
        "            batch_targets = batch_label_ids.to('cuda')\n",
        "\n",
        "            # Forward pass through the model to obtain predictions\n",
        "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, token_type_ids=batch_token_type_ids)\n",
        "\n",
        "            loss = criterion(outputs, batch_targets)\n",
        "\n",
        "            test_loss += loss.item() * batch_targets.size(0)\n",
        "            n_correct += (torch.argmax(outputs['predicts'], dim=1) == batch_targets).sum().item()\n",
        "            n_test += batch_targets.size(0)\n",
        "\n",
        "            predicted_labels = torch.argmax(outputs['predicts'], dim=1).cpu().numpy()\n",
        "            true_labels = batch_targets.cpu().numpy()\n",
        "\n",
        "            all_predicted.extend(predicted_labels)\n",
        "            all_targets.extend(true_labels)\n",
        "\n",
        "\n",
        "    # Calculate test loss and accuracy\n",
        "    test_loss /= n_test\n",
        "    test_acc = n_correct / n_test\n",
        "\n",
        "    # Calculate the classification report\n",
        "    classification_rep = classification_report(all_targets, all_predicted, target_names=class_names, digits=6, output_dict=True)\n",
        "\n",
        "    # Append the classification report (including all columns and rows) to the list\n",
        "    all_reports.append({'Model': model_path, **classification_rep, 'Test Loss': test_loss, 'Test Accuracy': test_acc})\n",
        "\n",
        "# Create a DataFrame from the list of reports\n",
        "report_df = pd.DataFrame(all_reports)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "report_df.to_csv('classification_report.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}